{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKsRDH5ZUdfasdv"
      },
      "source": [
        "# [Fall 2022] CS 4361 / 5361\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "overview"
      },
      "source": [
        "## **Before you start**\n",
        "\n",
        "Make a copy of this Colab by clicking on File > Save a Copy in Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nen9M7Vqj5cy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78e03f63-d647-43ec-e37c-d6e8903609a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Identifying Information:  Ricardo Sillas,88613474\n"
          ]
        }
      ],
      "source": [
        "student_name = \"Ricardo Sillas\"\n",
        "student_id = \"88613474\"\n",
        "\n",
        "print(\"Identifying Information: \",student_name+\",\"+student_id); # Don't change this line."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dM65L03qQcU7"
      },
      "source": [
        "### Problem 1\n",
        "\n",
        "Using a subset of the MNIST dataset, do the following:\n",
        "\n",
        "Implement method problem_1_part1 \n",
        "1. Build a logistic regression classifier on the training dataset \n",
        "2. With the trained model, predict the class of all examples in the test dataset \n",
        "3. Return predictions\n",
        "\n",
        "Implement method problem_1_part2\n",
        "1. Using the parameters real_digit_id, sim_digit_id, y_pred, and y_test, return the number of test images whose true class is real_digit_id, but were misclassified as sim_digit_id.\n",
        "\n",
        "For reproducibility purposes, build your classifier as follows (we'll use sklearn):\n",
        "\n",
        "    model = LogisticRegression(solver='lbfgs', max_iter=1000,random_state=0)\n",
        "    model.fit(x, y)\n",
        "\n",
        "Here's an example that shows how to get class predictions (actual class labels, not probabilities) using the trained model.\n",
        "\n",
        "    predictions = model.predict(x)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-KwGoTzR8Nu"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "def get_mnist_subset():\n",
        "  np.random.seed(0) # For reproducibility purposes\n",
        "  (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "  x_train = x_train.reshape((x_train.shape[0], -1)) / 255.0 # Flatten images\n",
        "  x_test = x_test.reshape((x_test.shape[0], -1)) / 255.0  # Flatten images\n",
        "\n",
        "  train_idx = np.random.permutation(np.arange(x_train.shape[0]))\n",
        "  test_idx = np.random.permutation(np.arange(x_test.shape[0]))\n",
        "\n",
        "  x_train, y_train = x_train[train_idx], y_train[train_idx]\n",
        "  x_test, y_test = x_test[test_idx], y_test[test_idx]\n",
        "\n",
        "  return x_train[:2000], y_train[:2000], x_test[:500], y_test[:500]\n",
        "   \n",
        "# Feel free to add helper methods, just don't change the signature\n",
        "def problem_1_part1():\n",
        "  x_train, y_train, x_test, y_test = get_mnist_subset()\n",
        "\n",
        "  model = LogisticRegression(solver='lbfgs', max_iter=1000, random_state=0)\n",
        "  model.fit(x_train, y_train)\n",
        "\n",
        "  return model.predict(x_test) # Feel free to remove/change this line\n",
        "\n",
        "\n",
        "def problem_1_part2(real_digit_id, sim_digit_id, y_pred, y_test):\n",
        "  return np.sum(y_pred[y_test == real_digit_id] == sim_digit_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1LO7aJ3ZsTw",
        "outputId": "baca9d4c-63b8-4cc2-874d-05fa24400372"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "Problem 1: 11.0 / 11.0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "# Test cases\n",
        "\n",
        "def test_p1():\n",
        "  problem_points = 11.0\n",
        "  num_tests = 8\n",
        "\n",
        "  passed_tests = 0\n",
        "\n",
        "  x_train, y_train, x_test, y_test = get_mnist_subset()\n",
        "\n",
        "  try:\n",
        "    y_pred = problem_1_part1()\n",
        "    passed_tests += (problem_1_part2(2, 8, y_pred, y_test) == 3)\n",
        "    passed_tests += (problem_1_part2(2, 0, y_pred, y_test) == 2)\n",
        "    passed_tests += (problem_1_part2(2, 7, y_pred, y_test) == 1)\n",
        "    passed_tests += (problem_1_part2(3, 9, y_pred, y_test) == 2)\n",
        "    passed_tests += (problem_1_part2(7, 1, y_pred, y_test) == 0)\n",
        "    passed_tests += (problem_1_part2(9, 7, y_pred, y_test) == 3)\n",
        "    passed_tests += (problem_1_part2(1, 8, y_pred, y_test) == 0)\n",
        "    passed_tests += (problem_1_part2(5, 0, y_pred, y_test) == 1)\n",
        "  except Exception as e:\n",
        "    print(\"Problem 1 - Exception thrown: \", e)\n",
        "\n",
        "  \n",
        "  # Need to pass at least 3 test cases to get partial credit\n",
        "  passed_tests = 0 if passed_tests < 3 else passed_tests\n",
        "  points = (passed_tests/num_tests) * problem_points\n",
        "\n",
        "  print(\"Problem 1:\", points, \"/\", problem_points)\n",
        "\n",
        "\n",
        "test_p1()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOVRzpDub72z"
      },
      "source": [
        "### Problem 2\n",
        "\n",
        "Using a subset of the MNIST dataset, do the following:\n",
        "\n",
        "Implement method problem_2_part1 \n",
        "1. Build a support vector machine classifier on the training dataset \n",
        "2. With the trained model, predict the class of all examples in the test dataset \n",
        "3. Return predictions\n",
        "\n",
        "Implement method problem_2_part2\n",
        "1. Using the parameters real_digit_id, sim_digit_id, y_pred, and y_test, return the number of test images whose predicted class is sim_digit_id, but whose true class is real_digit_id.\n",
        "\n",
        "For reproducibility purposes, build your classifier as follows (we'll use sklearn):\n",
        "\n",
        "    model = SVC(random_state=0)\n",
        "    model.fit(x, y)\n",
        "\n",
        "Here's an example that shows how to get class predictions (actual class labels, not probabilities) using the trained model.\n",
        "\n",
        "    predictions = model.predict(x)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXdUKbcRb3AP"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "def get_mnist_subset():\n",
        "  np.random.seed(0) # For reproducibility purposes\n",
        "  (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "  x_train = x_train.reshape((x_train.shape[0], -1)) / 255.0 # Flatten images\n",
        "  x_test = x_test.reshape((x_test.shape[0], -1)) / 255.0  # Flatten images\n",
        "\n",
        "  train_idx = np.random.permutation(np.arange(x_train.shape[0]))\n",
        "  test_idx = np.random.permutation(np.arange(x_test.shape[0]))\n",
        "\n",
        "  x_train, y_train = x_train[train_idx], y_train[train_idx]\n",
        "  x_test, y_test = x_test[test_idx], y_test[test_idx]\n",
        "\n",
        "  return x_train[:2000], y_train[:2000], x_test[:500], y_test[:500]\n",
        "   \n",
        "# Feel free to add helper methods, just don't change the signature\n",
        "def problem_2_part1():\n",
        "  x_train, y_train, x_test, y_test = get_mnist_subset()\n",
        "\n",
        "  model = SVC(random_state=0)\n",
        "  model.fit(x_train, y_train)\n",
        "\n",
        "  return model.predict(x_test)\n",
        "\n",
        "def problem_2_part2(real_digit_id, sim_digit_id, y_pred, y_test):\n",
        "  return np.sum(y_test[y_pred == sim_digit_id] == real_digit_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-huPEa9xiYip",
        "outputId": "ff8406b4-6c8f-4baf-aa6e-3993e8a9f8fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Problem 2: 11.0 / 11.0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "# Test cases\n",
        "\n",
        "def test_p2():\n",
        "  problem_points = 11.0\n",
        "  num_tests = 8\n",
        "\n",
        "  passed_tests = 0\n",
        "\n",
        "  try:\n",
        "    x_train, y_train, x_test, y_test = get_mnist_subset()\n",
        "    y_pred = problem_2_part1()\n",
        "    passed_tests += (problem_2_part2(2, 8, y_pred, y_test) == 0)\n",
        "    passed_tests += (problem_2_part2(2, 0, y_pred, y_test) == 2)\n",
        "    passed_tests += (problem_2_part2(2, 7, y_pred, y_test) == 0)\n",
        "    passed_tests += (problem_2_part2(4, 9, y_pred, y_test) == 1)\n",
        "    passed_tests += (problem_2_part2(7, 2, y_pred, y_test) == 3)\n",
        "    passed_tests += (problem_2_part2(5, 2, y_pred, y_test) == 2)\n",
        "    passed_tests += (problem_2_part2(9, 8, y_pred, y_test) == 1)\n",
        "    passed_tests += (problem_2_part2(5, 0, y_pred, y_test) == 1)\n",
        "  except Exception as e:\n",
        "    print(\"Problem 2 - Exception thrown: \", e)\n",
        "    \n",
        "  # Need to pass at least 3 test cases to get partial credit\n",
        "  passed_tests = 0 if passed_tests < 3 else passed_tests\n",
        "  points = (passed_tests/num_tests) * problem_points\n",
        "\n",
        "  print(\"Problem 2:\", points, \"/\", problem_points)\n",
        "\n",
        "\n",
        "test_p2()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6j7z2SOU0F8"
      },
      "source": [
        "### Problem 3\n",
        "\n",
        "Using a subset of the MNIST dataset, do the following:\n",
        "\n",
        "Implement method problem_3_part1 \n",
        "\n",
        "1. Using the training dataset, build a k-means model (k=20).\n",
        "2. Use the built model to assign cluster ids to all examples in the test dataset \n",
        "3. Return vector of cluster ids for test examples\n",
        "\n",
        "Implement method problem_3_part2\n",
        "1. Using parameters digit_id, y_test, y_test_cluster_ids,   return the id of the cluster that contains the largest number of test examples whose true class is digit_id.\n",
        "\n",
        "\n",
        "For reproducibility purposes, build your classifier as follows (we'll use sklearn):\n",
        "\n",
        "    model = KMeans(n_clusters=k, random_state=0)\n",
        "    model.fit(x)\n",
        "\n",
        "Here's an example that shows how to get the cluster assignments for the training examples: \n",
        "\n",
        "    model.labels_\n",
        "\n",
        "Here's an example that shows how to get cluster assignments of another dataset: \n",
        "    \n",
        "    model.predict(other_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LlRhhxle41_y"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "def get_mnist_subset():\n",
        "  np.random.seed(0) # For reproducibility purposes\n",
        "  (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "  x_train = x_train.reshape((x_train.shape[0], -1)) / 255.0 # Flatten images\n",
        "  x_test = x_test.reshape((x_test.shape[0], -1)) / 255.0  # Flatten images\n",
        "\n",
        "  train_idx = np.random.permutation(np.arange(x_train.shape[0]))\n",
        "  test_idx = np.random.permutation(np.arange(x_test.shape[0]))\n",
        "\n",
        "  x_train, y_train = x_train[train_idx], y_train[train_idx]\n",
        "  x_test, y_test = x_test[test_idx], y_test[test_idx]\n",
        "\n",
        "  return x_train[:2000], y_train[:2000], x_test[:500], y_test[:500]\n",
        "   \n",
        "# Feel free to add helper methods, just don't change the signature\n",
        "\n",
        "def problem_3_part1():\n",
        "  x_train, y_train, x_test, y_test = get_mnist_subset()\n",
        "\n",
        "  model = KMeans(n_clusters=20, random_state=0)\n",
        "  model.fit(x_train) \n",
        "\n",
        "  return model.predict(x_test)\n",
        "\n",
        "\n",
        "def problem_3_part2(digit_id, y_test, y_test_cluster_ids):\n",
        "  curr_max = 0\n",
        "  index = 0\n",
        "\n",
        "  for i in range(len(y_test)):\n",
        "    curr = np.sum(y_test_cluster_ids[y_test == digit_id] == i)\n",
        "    \n",
        "    if curr > curr_max:\n",
        "      curr_max = curr\n",
        "      index = i\n",
        "\n",
        "  return index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcSoXP7roKaT",
        "outputId": "428133d1-3e57-4675-a5cb-bee8b9461570"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Problem 3: 11.0 / 11.0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "# Test cases\n",
        "\n",
        "def test_p3():\n",
        "  problem_points = 11.0\n",
        "  num_tests = 10\n",
        "\n",
        "  passed_tests = 0\n",
        "\n",
        "  try:\n",
        "    x_train, y_train, x_test, y_test = get_mnist_subset()\n",
        "    y_test_cluster_ids = problem_3_part1()\n",
        "\n",
        "    s = [1, 18, 10, 14, 18, 12, 4, 3, 9, 7]\n",
        "    for i in range(num_tests):\n",
        "      passed_tests += (problem_3_part2(i, y_test, y_test_cluster_ids) == s.pop())\n",
        "  except Exception as e:\n",
        "      print(\"Problem 3 - Exception thrown: \", e)\n",
        "  \n",
        "  # Need to pass at least 3 test cases to get partial credit\n",
        "  passed_tests = 0 if passed_tests < 3 else passed_tests\n",
        "  points = (passed_tests/num_tests) * problem_points\n",
        "\n",
        "  print(\"Problem 3:\", points, \"/\", problem_points)\n",
        "\n",
        "\n",
        "test_p3()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AafrC0Yee1dl"
      },
      "source": [
        "### Problem 4\n",
        "\n",
        "Change the architecture of the convolutional neural network defined in build_model (add/remove layers, change the number of units/filters, etc.) so it achieves an accuracy of 0.4 or better on the validation dataset. We will use a subset of CIFAR-10. You are not allowed to change any code outside of the build_model method (you can't change the number of epochs, mini-batch size, etc.).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ysVgHe_KfS92"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow import keras\n",
        "\n",
        "# Change the model architecture defined in this method\n",
        "def build_model(input_shape, num_classes=10):  \n",
        "  model = keras.Sequential()\n",
        "\n",
        "  model.add(keras.layers.Conv2D(filters=8, kernel_size=(3, 1), padding='same',\n",
        "                  input_shape=input_shape))\n",
        "  \n",
        "  model.add(keras.layers.Activation('relu'))\n",
        "  model.add(keras.layers.MaxPooling2D(pool_size=(1, 1)))\n",
        " \n",
        "  model.add(keras.layers.Flatten())\n",
        "\n",
        "  model.add(keras.layers.Dense(units=500))\n",
        "  model.add(keras.layers.Activation('relu'))\n",
        "\n",
        "  model.add(keras.layers.Dense(units=500))\n",
        "  model.add(keras.layers.Activation('relu'))\n",
        "\n",
        "  model.add(keras.layers.Dense(units=300))\n",
        "  model.add(keras.layers.Activation('softmax'))\n",
        "  \n",
        "  return model\n",
        "\n",
        "\n",
        "# Do not change the code inside of this method\n",
        "def get_cifar10_subset():\n",
        "  np.random.seed(0) # For reproducibility purposes\n",
        "  (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "  x_train = x_train / 255.0 \n",
        "  x_test = x_test / 255.0  \n",
        "\n",
        "  train_idx = np.random.permutation(np.arange(x_train.shape[0]))\n",
        "  test_idx = np.random.permutation(np.arange(x_test.shape[0]))\n",
        "\n",
        "  x_train, y_train = x_train[train_idx], y_train[train_idx]\n",
        "  x_test, y_test = x_test[test_idx], y_test[test_idx]\n",
        "\n",
        "  return x_train[:2000], y_train[:2000], x_test[:500], y_test[:500]\n",
        "\n",
        "\n",
        "# Do not change the code inside of this method\n",
        "def train_model():\n",
        "  x_train, y_train, x_test, y_test = get_cifar10_subset()\n",
        "\n",
        "  tf.keras.backend.clear_session()\n",
        "  np.random.seed(0)\n",
        "  tf.random.set_seed(0)\n",
        "\n",
        "  model = build_model(x_train[0].shape)\n",
        "  \n",
        "  loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
        "  optimizer = tf.keras.optimizers.Adam()\n",
        "  \n",
        "  model.compile(loss=loss,\n",
        "              optimizer=optimizer,\n",
        "              metrics=['accuracy'])\n",
        "  \n",
        "  h = model.fit(\n",
        "    x = x_train,   \n",
        "    y = y_train,  \n",
        "    epochs=5,    \n",
        "    batch_size=64, \n",
        "    validation_data=(x_test, y_test),\n",
        "    verbose=1   \n",
        "  )\n",
        "\n",
        "  val_acc = h.history['val_accuracy'][-1]\n",
        "  print(\"Final validation accuracy:\", val_acc)\n",
        "  return val_acc\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sp2EAHPNqPkI",
        "outputId": "b4cf5a58-cd04-4eb6-ec88-fa9c714d4000"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 4s 0us/step\n",
            "Epoch 1/5\n",
            "32/32 [==============================] - 4s 102ms/step - loss: 2.5453 - accuracy: 0.1715 - val_loss: 2.1057 - val_accuracy: 0.1940\n",
            "Epoch 2/5\n",
            "32/32 [==============================] - 3s 86ms/step - loss: 1.8405 - accuracy: 0.3365 - val_loss: 1.9088 - val_accuracy: 0.3100\n",
            "Epoch 3/5\n",
            "32/32 [==============================] - 3s 99ms/step - loss: 1.6790 - accuracy: 0.4155 - val_loss: 1.7756 - val_accuracy: 0.3660\n",
            "Epoch 4/5\n",
            "32/32 [==============================] - 5s 152ms/step - loss: 1.4663 - accuracy: 0.4880 - val_loss: 1.8173 - val_accuracy: 0.3600\n",
            "Epoch 5/5\n",
            "32/32 [==============================] - 3s 95ms/step - loss: 1.3028 - accuracy: 0.5470 - val_loss: 1.7538 - val_accuracy: 0.4020\n",
            "Final validation accuracy: 0.4020000100135803\n",
            "Problem 4: 11.0 / 11.0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "# Test cases\n",
        "\n",
        "def test_p4():\n",
        "  problem_points = 11.0\n",
        "  num_tests = 1\n",
        "\n",
        "  try:\n",
        "    passed_tests = int(train_model() >= 0.4)\n",
        "  except Exception as e:\n",
        "    print(\"Problem 4 - Exception thrown: \", e)\n",
        "\n",
        "  points = (passed_tests/num_tests) * problem_points\n",
        "\n",
        "  print(\"Problem 4:\", points, \"/\", problem_points)\n",
        "\n",
        "\n",
        "test_p4()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaSkeeyRBV3X"
      },
      "source": [
        "### Problem 5\n",
        "\n",
        "Given the training dataset for a binary classification task (x_train, y_train), return the column index of the binary attribute with the highest information gain. Notice that you are given the implementation of the gain function. You just need to use it to find the index of the attribute that maximizes it\n",
        "\n",
        "Make the following assumptions:\n",
        "1. x_train is a numpy matrix of size (num_examples, num_attributes)\n",
        "2. x_train contains only 0s and 1s\n",
        "3. y_train is a numpy vector of size (num_examples) that contains only 0s and 1s\n",
        "\n",
        "Formulas (you do not need them to solve this problem):\n",
        "\n",
        "$Entropy(S) = -p_+ log_2(p_+) - p_- log_2(p_-) $\n",
        "\n",
        "$Gain(S, A) = Entropy(S) - \\sum_{v \\in \\{0,1\\}} \\frac{|S_v|}{|S|}Entropy(S_v)$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HS5TJvGdG_ul"
      },
      "outputs": [],
      "source": [
        "# Feel free to add helper methods, just don't change the signature\n",
        "# of the gain method\n",
        "\n",
        "# Feel free to add helper methods, just don't change the signature\n",
        "# of the gain method\n",
        "import numpy as np\n",
        "\n",
        "def entropy(pp, pm):\n",
        "  return -pp * np.log2(pp+0.000001) - pm * np.log2(pm+0.000001)\n",
        "\n",
        "def gain(x_train, y_train, att_col):\n",
        "  x_0 = x_train[:, att_col] == 0\n",
        "  x_1 = x_train[:, att_col] == 1\n",
        "\n",
        "  y_0 = y_train[x_0]\n",
        "  y_1 = y_train[x_1]\n",
        "\n",
        "  e_s = entropy(np.sum(y_train==1)/y_train.shape[0], np.sum(y_train==0)/y_train.shape[0])\n",
        "\n",
        "  e_0 = entropy(np.sum(y_0==1)/y_0.shape[0], np.sum(y_0==0)/y_0.shape[0])\n",
        "  e_1 = entropy(np.sum(y_1==1)/y_1.shape[0], np.sum(y_1==0)/y_1.shape[0])\n",
        "\n",
        "  w_0 = y_0.shape[0] / y_train.shape[0]\n",
        "  w_1 = y_1.shape[0] / y_train.shape[0]\n",
        "\n",
        "  return e_s - w_0 * e_0 - w_1 * e_1\n",
        "\n",
        "def best_attribute(x_train, y_train):\n",
        "  gains = []\n",
        "  for i in range(x_train.shape[1]):\n",
        "    gains.append(gain(x_train,y_train,i))\n",
        "  \n",
        "  curr_max = 0\n",
        "  curr_val = 0\n",
        "  for i in range(len(gains)):\n",
        "    curr = gains[i]\n",
        "    if curr > curr_max:\n",
        "      curr_max = curr\n",
        "      index = i\n",
        "  return index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7vmDlKeHmK7",
        "outputId": "4d97cc7a-1460-40ec-ab25-d627907d376d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Problem 5: 11.0 / 11.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: RuntimeWarning: invalid value encountered in long_scalars\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "# Test cases\n",
        "\n",
        "def test_gain():\n",
        "  problem_points = 11.0\n",
        "  num_tests = 10\n",
        "\n",
        "  passed_tests = 0\n",
        "\n",
        "  for i in range(num_tests):\n",
        "    x_train = np.random.randint(2, size=(10,20))\n",
        "    x_train = np.float32(x_train)\n",
        "    idx = (i+i)%20\n",
        "    idx = 1 if idx == 0 else idx\n",
        "    y_train = x_train[:,idx]\n",
        "    att = best_attribute(x_train, y_train)\n",
        "    \n",
        "    col = x_train[:,att]\n",
        "    \n",
        "\n",
        "    passed_tests += int((y_train==col).all())\n",
        "\n",
        "  points = (passed_tests/num_tests) * problem_points\n",
        "  if points > 8.7:\n",
        "    points = problem_points\n",
        "\n",
        "  print(\"Problem 5:\", points, \"/\", problem_points)\n",
        "\n",
        "\n",
        "test_gain()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkXziJOCpjwX"
      },
      "source": [
        "### Problem 6\n",
        "\n",
        "Given the training (x_train, y_train) and testing (x_test, y_test) datasets for a binary classification task, build multiple k-nn classifiers on the training dataset using the following values for k: 1, 3, 5, 7, and return the value of k that results in the highest accuracy on the test dataset. \n",
        "\n",
        "Make the following assumptions:\n",
        "1. x_train is a numpy matrix of size (num_training_examples, num_attributes)\n",
        "2. x_test is a numpy matrix of size (num_test_examples, num_attributes)\n",
        "3. y_train is a numpy vector of size (num_training_examples) that contains only 0s and 1s\n",
        "4. y_test is a numpy vector of size (num_test_examples) that contains only 0s and 1s\n",
        "\n",
        "Here's an example on how you can build a k-nn classifier where k=3:\n",
        "\n",
        "    model = KNeighborsClassifier(n_neighbors=3)\n",
        "    model.fit(x, y)\n",
        "\n",
        "Here's an example on how you can test the classifier\n",
        "\n",
        "    predictions = model.predict(x)\n",
        "    accuracy = accuracy_score(true_labels, predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qIAt7kRppjwX"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Feel free to add helper methods, just don't change the signature\n",
        "# of the get_best_k method\n",
        "def get_best_k(x_train, y_train, x_test, y_test):\n",
        "  k =[1,3,5,7]\n",
        "  curr_highest = 0\n",
        "  best = 0\n",
        "\n",
        "  for i in k:\n",
        "    model = KNeighborsClassifier(n_neighbors=i)\n",
        "    model.fit(x_train, y_train)\n",
        "    predictions = model.predict(x_test)\n",
        "    accuracy = accuracy_score(y_test, predictions)\n",
        "\n",
        "    if accuracy > curr_highest:\n",
        "      curr_highest = accuracy\n",
        "      best = i\n",
        "\n",
        "  return best "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEPplukunWVU",
        "outputId": "a8f7dda8-a5e0-4056-d567-908d01a5ad2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Problem 6: 11.0 / 11.0\n"
          ]
        }
      ],
      "source": [
        "# Test cases\n",
        "\n",
        "def test_knn():\n",
        "  problem_points = 11.0\n",
        "  x_train = np.array([\n",
        "                      [1, 1, 1],\n",
        "                      [1, 1, 0],\n",
        "                      [1, 0, 1],\n",
        "                      [1, 0, 0],\n",
        "                      [0, 1, 1],\n",
        "                      [0, 1, 0],\n",
        "                      [0, 0, 1],\n",
        "                      [0, 0, 0],\n",
        "                    ], dtype=np.float32)\n",
        "  \n",
        "  y_train = np.array([0,0,1,0,1,1,1,1], dtype=np.float32)\n",
        "\n",
        "  num_tests = 3\n",
        "  passed_tests = 0\n",
        "\n",
        "  passed_tests += int(get_best_k(x_train, y_train, x_train, y_train)==1)\n",
        "\n",
        "  passed_tests += int(get_best_k(x_train, y_train, x_train, 1-y_train)==7)\n",
        "\n",
        "  y_test = np.array([1,0,1,1,0,1,0,1], dtype=np.float32)\n",
        "\n",
        "  passed_tests += int(get_best_k(x_train, y_train, x_train, y_test)==5)\n",
        "\n",
        "  print(\"Problem 6:\", (passed_tests/num_tests) * problem_points, \"/\", problem_points)\n",
        "\n",
        "  \n",
        "\n",
        "test_knn()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3w97KgZ8pjwY"
      },
      "source": [
        "### Problem 7\n",
        "\n",
        "Given the predictions made by a model (y_pred) for a regression task, the corresponding true y values (y_test), and the true y values of the training dataset (y_train), do the following:\n",
        "\n",
        "1. Compute mae_pred: Mean-squared error using y_test and y_pred\n",
        "2. Compute mae_mean: Mean-squared error using y_test and y_mean, where y_mean represents the predictions made by a dummy model that always predicts the mean of y_train for any test example.\n",
        "3. Return mae_pred - mae_mean\n",
        "\n",
        "Make the following assumptions:\n",
        "1. y_pred is a numpy vector of size (num_test_examples) that contains floating point numbers\n",
        "2. y_test is a numpy vector of size (num_test_examples) that contains floating point numbers\n",
        "3. y_train is a numpy vector of size (num_training_examples) that contains floating point numbers\n",
        "\n",
        "Feel free to use the method mean_squared_error. Here's an example on how to use it:\n",
        "\n",
        "    y_true = [3, -0.5, 2, 7]\n",
        "    y_pred = [2.5, 0.0, 2, 8]\n",
        "    mae_pred = mean_squared_error(y_true, y_pred)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5B7JyQsgV2PH"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Feel free to add helper methods, just don't change the signature\n",
        "# of the is_better_than_baseline method\n",
        "def pred_vs_baseline(y_train, y_test, y_pred):  \n",
        "  mae_pred = mean_squared_error(y_test, y_pred)\n",
        "  y_mean = np.ones(len(y_train)) * np.mean(y_train)\n",
        "  mae_mean = mean_squared_error(y_test, y_mean)\n",
        "\n",
        "  return mae_pred-mae_mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZrZ96JDnaS7",
        "outputId": "f626fce5-dbe4-4575-b7e7-e7fe0b8e69f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Problem 7: 11.0 / 11.0\n"
          ]
        }
      ],
      "source": [
        "# Test cases\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def test_mvb():\n",
        "  problem_points = 11.0\n",
        "\n",
        "  num_tests = 10\n",
        "  passed_tests = 0\n",
        "  eps = 0.0001\n",
        "\n",
        "  for i in range(num_tests):\n",
        "    one, two, three = np.random.random(size=10), np.random.random(size=10), np.random.random(size=10)\n",
        "    \n",
        "    if abs(pred_vs_baseline(one, two, three) - (((two - three)**2).mean() - ((two - one.mean())**2).mean())) <= eps:\n",
        "      passed_tests += 1\n",
        "\n",
        "\n",
        "  print(\"Problem 7:\", (passed_tests/num_tests) * problem_points, \"/\", problem_points)\n",
        "\n",
        "test_mvb()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1JNV1cvWJof"
      },
      "source": [
        "###  Problem 8\n",
        "\n",
        "Given a training dataset (x_train) count and return the number of attributes that do **not** have the same value for all examples in x_train.\n",
        "\n",
        "Make the following assumptions:\n",
        "1. x_train is a numpy matrix of size (num_examples, num_attributes)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qx2zWYpKW6SI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Feel free to add helper methods, just don't change the signature\n",
        "# of the count_same_val_atts method\n",
        "def count_diif_val_atts(x_train):\n",
        "  count = 0\n",
        "  \n",
        "  for i in range(x_train.shape[1]):\n",
        "    col = x_train[:,i]\n",
        "    count = count+0 if all(element == col[0] for element in col) else count+1\n",
        "\n",
        "  return count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2doM-BMnfip",
        "outputId": "354e8303-cc41-4d54-c2ea-704345aa1d4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Problem 8: 11.0 / 11.0\n"
          ]
        }
      ],
      "source": [
        "# Test cases\n",
        "\n",
        "def test_attc():\n",
        "  problem_points = 11.0\n",
        "  num_tests = 3\n",
        "  passed_tests = 0\n",
        "\n",
        "  x_train = np.array([\n",
        "                      [1, 0, 0, 2, 3, 5],\n",
        "                      [1, 1, 0, 2, 3, 5],\n",
        "                      [1, 0, 0, 1, 4, 5],\n",
        "                    ], dtype=np.float32)\n",
        "  \n",
        "  passed_tests += int(count_diif_val_atts(x_train) == 3)\n",
        "\n",
        "  x_train = np.array([\n",
        "                      [1, -1, 0, 2, 3, 5],\n",
        "                      [2, -1, 0, 2, 3, 5],\n",
        "                      [1, -1, 0, 2, 4, 5],\n",
        "                    ], dtype=np.float32)\n",
        "  \n",
        "  passed_tests += int(count_diif_val_atts(x_train) == 2)\n",
        "\n",
        "  x_train = np.array([\n",
        "                      [1, -1, 0, 2, 3, 0],\n",
        "                      [2, -1, 0, 5, 3, 5],\n",
        "                      [1, -2, 1, 2, 4, 5],\n",
        "                    ], dtype=np.float32)\n",
        "  \n",
        "  passed_tests += int(count_diif_val_atts(x_train) == 6)  \n",
        "\n",
        "  print(\"Problem 8:\", (passed_tests/num_tests) * problem_points, \"/\", problem_points)\n",
        "  \n",
        "test_attc()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pvc8YZjiyMO5"
      },
      "source": [
        "## How to Submit This Assignment\n",
        "\n",
        "1. File > Download .ipynb\n",
        "2. Go to Blackboard, find the lab submission page, and upload the .ipynb file you just downloaded."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Pvc8YZjiyMO5"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}